{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80e9d54",
   "metadata": {},
   "source": [
    "Pick 30 questions to run a debate on.\n",
    "30 debates will be run with gemini-3-pro\n",
    "30 debates will be run with grok-4-fast.\n",
    "\n",
    "10 of these debates will be used for validation. Will use them to:\n",
    "- Tune debater prompts\n",
    "- Tune judge prompts\n",
    "- Set up the human eval script (will collect certainty after every turn)\n",
    "\n",
    "Will choose the questions that were hardest across all models BUT were correct for grok-4-fast.\n",
    "\n",
    "Notes:\n",
    "- Expect positive regression from QA->Debate simply because choosing poor QA outcomes. Strong baseline will be the judge model's performance with itself as debater.\n",
    "- Outcomes: \n",
    "    - If the judges are able to do it, we have a strong result (doubt it)\n",
    "    - If judges are unable but I am able, we have a clear target for improvement (training judge with process supervision)\n",
    "    - If I am unable to do it, we've got nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "729a6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the hardest questions\n",
    "\n",
    "from analysis_utils import prepare_df\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f233e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = prepare_df(['qa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a941d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('Idavidrein/gpqa', 'gpqa_diamond')['train']\n",
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df = dataset_df.rename({'Question': 'question', 'High-level domain': 'high_level_domain'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a40b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = qa_df.merge(dataset_df[['question', 'high_level_domain']], left_on=['question_qa'], right_on=['question'], how='left', suffixes=('', '_dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99507f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run_id_qa', 'datetime_qa', 'question_idx_qa', 'question_qa',\n",
       "       'options_qa', 'correct_idx_qa', 'raw_model_response_qa',\n",
       "       'parsed_model_response_qa', 'prompt_qa', 'token_usage_qa',\n",
       "       'record_id_qa', 'prompt_template_qa', 'internal_model_reasoning_qa',\n",
       "       'internal_model_reasoning_details_qa', 'success_qa', 'error_message_qa',\n",
       "       'config_dataset_name_qa', 'config_dataset_subset_qa',\n",
       "       'config_dataset_split_qa', 'config_model_name_qa',\n",
       "       'config_temperature_qa', 'config_num_questions_qa',\n",
       "       'config_random_seed_qa', 'config_num_choices_qa',\n",
       "       'config_max_threads_qa', 'config_specific_question_idxs_qa',\n",
       "       'config_rerun_qa', 'config_lenient_parsing_qa', 'config_max_tokens_qa',\n",
       "       'config_reasoning_effort_qa', 'config_reasoning_max_tokens_qa',\n",
       "       'options_str_qa', 'parsed_answer_qa', 'is_correct_qa', 'question',\n",
       "       'high_level_domain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0431206f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2090, 36)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_filters = {\n",
    "    'config_dataset_name_qa': 'Idavidrein/gpqa',\n",
    "    'config_num_choices_qa': 2,\n",
    "    'config_random_seed_qa': 42\n",
    "}\n",
    "\n",
    "temp_df = qa_df.copy()\n",
    "for filter in qa_filters:\n",
    "    temp_df = temp_df[temp_df[filter] == qa_filters[filter]]\n",
    "temp_df.shape\n",
    "\n",
    "# drop some judge models with low sample counts\n",
    "temp_df = temp_df[~temp_df['config_model_name_qa'].isin(['qwen/qwen3-235b-a22b', 'qwen/qwen3-8b', 'google/gemini-3-pro-preview'])]\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1868486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config_model_name_qa\n",
       "x-ai/grok-4-fast                     198\n",
       "openai/gpt-4o-mini                   198\n",
       "openai/gpt-3.5-turbo                 197\n",
       "google/gemma-3-27b-it                196\n",
       "qwen/qwen-2.5-72b-instruct           195\n",
       "qwen/qwen3-14b                       192\n",
       "qwen/qwen-2.5-7b-instruct            191\n",
       "meta-llama/llama-3-8b-instruct       189\n",
       "google/gemma-3-12b-it                182\n",
       "qwen/qwen3-32b                       177\n",
       "meta-llama/llama-3.1-70b-instruct    175\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['config_model_name_qa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf137010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config_model_name_qa\n",
       "x-ai/grok-4-fast                     186\n",
       "openai/gpt-4o-mini                   186\n",
       "openai/gpt-3.5-turbo                 185\n",
       "qwen/qwen-2.5-72b-instruct           184\n",
       "google/gemma-3-27b-it                184\n",
       "qwen/qwen3-14b                       180\n",
       "qwen/qwen-2.5-7b-instruct            179\n",
       "meta-llama/llama-3-8b-instruct       178\n",
       "google/gemma-3-12b-it                171\n",
       "qwen/qwen3-32b                       165\n",
       "meta-llama/llama-3.1-70b-instruct    164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any where grok-4-fast got it wrong (less chance of debate working there)\n",
    "valid_question_idxs = temp_df[(temp_df['config_model_name_qa'] == 'x-ai/grok-4-fast') & (temp_df['is_correct_qa'] == True)]['question_idx_qa']\n",
    "len(valid_question_idxs)\n",
    "\n",
    "temp_df = temp_df[temp_df['question_idx_qa'].isin(valid_question_idxs)]\n",
    "temp_df['config_model_name_qa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a95a4c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/xmd1jn1s1gg47vfyv_n8g3xh0000gn/T/ipykernel_14605/2353016564.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hardest_qs = temp_df.groupby(by=['question_idx_qa']).apply(lambda x: x['is_correct_qa'].mean()).sort_values().head(30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "question_idx_qa\n",
       "176.0    0.090909\n",
       "179.0    0.222222\n",
       "192.0    0.272727\n",
       "136.0    0.272727\n",
       "173.0    0.363636\n",
       "123.0    0.363636\n",
       "129.0    0.363636\n",
       "56.0     0.444444\n",
       "9.0      0.444444\n",
       "178.0    0.454545\n",
       "3.0      0.454545\n",
       "169.0    0.454545\n",
       "57.0     0.454545\n",
       "55.0     0.454545\n",
       "180.0    0.500000\n",
       "84.0     0.500000\n",
       "165.0    0.545455\n",
       "70.0     0.545455\n",
       "103.0    0.545455\n",
       "146.0    0.545455\n",
       "119.0    0.545455\n",
       "68.0     0.555556\n",
       "154.0    0.600000\n",
       "49.0     0.600000\n",
       "34.0     0.600000\n",
       "95.0     0.636364\n",
       "150.0    0.636364\n",
       "108.0    0.636364\n",
       "143.0    0.636364\n",
       "194.0    0.636364\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = temp_df[temp_df['high_level_domain'] == 'Physics']\n",
    "\n",
    "hardest_qs = temp_df.groupby(by=['question_idx_qa']).apply(lambda x: x['is_correct_qa'].mean()).sort_values().head(30)\n",
    "hardest_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a60f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173, 45, 176, 118, 179, 110, 56, 92, 164, 129, 149, 32, 75, 125, 142, 63, 33, 196, 9, 192, 182, 76, 139, 97, 105, 189, 57, 136, 22, 123]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "shuffled = random.sample(hardest_qs.keys().astype(int).tolist(), 30, )\n",
    "\n",
    "print(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3547eaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173, 45, 176, 118, 179, 110, 56, 92, 164, 129]\n"
     ]
    }
   ],
   "source": [
    "validation = shuffled[:10]\n",
    "test = shuffled[10:]\n",
    "\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ede3dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 32, 75, 125, 142, 63, 33, 196, 9, 192, 182, 76, 139, 97, 105, 189, 57, 136, 22, 123]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50c9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_debate_NEW_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
