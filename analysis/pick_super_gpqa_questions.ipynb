{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80e9d54",
   "metadata": {},
   "source": [
    "Pick 25 questions to run a debate on from super gpqa\n",
    "25 debates will be run with grok-4-fast.\n",
    "\n",
    "5 of these debates will be used for validation. Will use them to:\n",
    "- Tune the human data collection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "729a6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the hardest questions\n",
    "\n",
    "from analysis_utils import prepare_df\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f233e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = prepare_df(['qa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "014248eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 37)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physics_df = qa_df[(qa_df['config_dataset_filters.field_qa'] == 'Physics') & (qa_df['config_dataset_filters.difficulty_qa'] == 'hard')]\n",
    "# physics_df = qa_df[(qa_df['config_dataset_filters.field_qa'] == 'Physics') & (qa_df['config_dataset_filters.difficulty_qa'] == 'easy')]\n",
    "\n",
    "\n",
    "physics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "964870cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run_id_qa', 'datetime_qa', 'question_idx_qa', 'question_qa',\n",
       "       'options_qa', 'correct_idx_qa', 'raw_model_response_qa',\n",
       "       'parsed_model_response_qa', 'prompt_qa', 'token_usage_qa',\n",
       "       'record_id_qa', 'prompt_template_qa', 'internal_model_reasoning_qa',\n",
       "       'internal_model_reasoning_details_qa', 'success_qa', 'error_message_qa',\n",
       "       'config_dataset_name_qa', 'config_dataset_subset_qa',\n",
       "       'config_dataset_split_qa', 'config_model_name_qa',\n",
       "       'config_temperature_qa', 'config_num_questions_qa',\n",
       "       'config_random_seed_qa', 'config_num_choices_qa',\n",
       "       'config_max_threads_qa', 'config_specific_question_idxs_qa',\n",
       "       'config_rerun_qa', 'config_lenient_parsing_qa', 'config_max_tokens_qa',\n",
       "       'config_reasoning_effort_qa', 'config_reasoning_max_tokens_qa',\n",
       "       'config_dataset_filters.field_qa',\n",
       "       'config_dataset_filters.difficulty_qa',\n",
       "       'config_dataset_filters.category_qa', 'options_str_qa',\n",
       "       'parsed_answer_qa', 'is_correct_qa'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1ca67bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grok_correct_idxs = physics_df[(physics_df['is_correct_qa']==True)& (physics_df['config_model_name_qa']=='x-ai/grok-4-fast')]['question_idx_qa']\n",
    "len(grok_correct_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb174127",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_wrong_idxs = physics_df[physics_df['question_idx_qa'].isin(grok_correct_idxs) & (physics_df['config_model_name_qa'] == 'openai/gpt-4o-mini') & (physics_df['is_correct_qa']==False)]['question_idx_qa']\n",
    "len(gpt_4o_mini_wrong_idxs)\n",
    "\n",
    "gpt_4o_mini_wrong_idxs = gpt_4o_mini_wrong_idxs.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc54814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25803, 5627, 4880, 10347, 10561]\n",
      "[19485, 18347, 12849, 14191, 25441, 861, 12266, 12563, 24926, 6143, 5868, 21169, 6120, 10361, 5136, 4049, 23588, 5021, 19402, 7567, 1904, 11812, 14101, 5984, 11661, 14195]\n"
     ]
    }
   ],
   "source": [
    "validation_idxs = gpt_4o_mini_wrong_idxs[:5]\n",
    "test_idxs = gpt_4o_mini_wrong_idxs[5:35]\n",
    "\n",
    "print(validation_idxs)\n",
    "print(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d02cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0340e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_debate_NEW_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
