{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf977baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb27feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from analysis.analysis_utils import *\n",
    "\n",
    "human_verdicts_file = '../results/human/human_verdicts.jsonl'\n",
    "human_df = pd.read_json(human_verdicts_file, lines=True)\n",
    "\n",
    "run_id = '0vayjit'\n",
    "human_df = human_df[human_df['run_id'] == run_id]\n",
    "\n",
    "debates_df = prepare_df(['debates'])\n",
    "debates_df = debates_df[debates_df['run_id_debates'] == run_id]\n",
    "\n",
    "merged = human_df.merge(debates_df[['run_id_debates', 'record_id_debates', 'correct_idx_debates']], \n",
    "                        left_on=['run_id', 'record_id'], \n",
    "                        right_on=['run_id_debates', 'record_id_debates'])\n",
    "\n",
    "merged['verdict'] = merged['verdict'].astype(int)\n",
    "merged['correct_idx_debates'] = merged['correct_idx_debates'].astype(int)\n",
    "merged['confidence'] = merged['confidence'].astype(float) / 100.0\n",
    "\n",
    "merged['is_correct'] = merged['verdict'] == merged['correct_idx_debates']\n",
    "\n",
    "accuracy = merged['is_correct'].mean()\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "def compute_brier_score(merged_df):\n",
    "    brier_scores = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        verdict = row['verdict']\n",
    "        confidence = row['confidence']\n",
    "        correct = row['correct_idx_debates']\n",
    "        \n",
    "        if verdict == correct:\n",
    "            predicted_prob_correct = confidence\n",
    "        else:\n",
    "            predicted_prob_correct = 1.0 - confidence\n",
    "        \n",
    "        actual = 1.0\n",
    "        brier_score = (predicted_prob_correct - actual) ** 2\n",
    "        brier_scores.append(brier_score)\n",
    "    \n",
    "    return np.mean(brier_scores)\n",
    "\n",
    "brier_score = compute_brier_score(merged)\n",
    "print(f\"Brier Score: {brier_score:.4f}\")\n",
    "\n",
    "baseline_brier = 0.25\n",
    "print(f\"Baseline Brier Score (always predict 0.5): {baseline_brier:.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.bar(['Correct', 'Incorrect'], [merged['is_correct'].sum(), (~merged['is_correct']).sum()], \n",
    "        color=['green', 'red'], alpha=0.5)\n",
    "ax1.set_ylabel('Count', fontsize=16)\n",
    "# ax1.set_title(f'Accuracy: {accuracy:.1%}', fontsize=14, fontweight='bold')\n",
    "ax1.set_title(f'Accuracy',  fontsize=16, fontweight='bold')\n",
    "\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "correct_conf = merged[merged['is_correct']]['confidence'] * 100\n",
    "incorrect_conf = merged[~merged['is_correct']]['confidence'] * 100\n",
    "\n",
    "ax2.hist([correct_conf, incorrect_conf], bins=10, alpha=0.5, \n",
    "         label=['Correct Verdict', 'Incorrect Verdict'], color=['green', 'red'], edgecolor='black')\n",
    "ax2.set_xlabel('Confidence (%)', fontsize=16)\n",
    "ax2.set_title('Confidence', fontsize=16, fontweight='bold')\n",
    "ax2.legend(fontsize=18)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='both', labelsize=14)\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBrier Score: {brier_score:.4f}\")\n",
    "print(f\"Baseline Brier Score: {baseline_brier:.4f}\")\n",
    "print(f\"Improvement: {baseline_brier - brier_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904f160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_debate_NEW_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
