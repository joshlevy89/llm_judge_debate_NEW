{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_file = \"results/qa_results.jsonl\"\n",
        "\n",
        "# Filters (set to None to include all)\n",
        "FILTER_MODEL_NAME = \"openai/gpt-4o-mini\"\n",
        "FILTER_DATASET_NAME = \"Idavidrein/gpqa\"\n",
        "FILTER_DATASET_SUBSET = \"gpqa_diamond\"\n",
        "FILTER_DATASET_SPLIT = \"train\"\n",
        "FILTER_RANDOM_SEED = 42\n",
        "FILTER_NUM_CHOICES = 2\n",
        "\n",
        "# Deduplication settings\n",
        "REMOVE_DUPLICATES = True  # Remove duplicate runs (same question/config, keeping latest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_idx</th>\n",
              "      <th>question</th>\n",
              "      <th>options</th>\n",
              "      <th>correct_idx</th>\n",
              "      <th>raw_model_response</th>\n",
              "      <th>parsed_model_response</th>\n",
              "      <th>prompt</th>\n",
              "      <th>parsed_answer</th>\n",
              "      <th>is_correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>You have an interesting drought-resistant cult...</td>\n",
              "      <td>[Mutant 3, Mutant 2]</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;BEGIN FINAL ANSWER&gt;\\nAnswer: 1\\nConfidence: 9...</td>\n",
              "      <td>{'is_valid': True, 'answer': 1, 'confidence': ...</td>\n",
              "      <td>Answer the following question. You must choose...</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>163</td>\n",
              "      <td>Astronomers are studying two binary star syste...</td>\n",
              "      <td>[~ 0.4, ~ 0.6]</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;THINKING&gt;\\nTo determine by what factor system...</td>\n",
              "      <td>{'is_valid': True, 'answer': 0, 'confidence': ...</td>\n",
              "      <td>Answer the following question. You must choose...</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_idx                                           question  \\\n",
              "0            28  You have an interesting drought-resistant cult...   \n",
              "1           163  Astronomers are studying two binary star syste...   \n",
              "\n",
              "                options  correct_idx  \\\n",
              "0  [Mutant 3, Mutant 2]            1   \n",
              "1        [~ 0.4, ~ 0.6]            0   \n",
              "\n",
              "                                  raw_model_response  \\\n",
              "0  <BEGIN FINAL ANSWER>\\nAnswer: 1\\nConfidence: 9...   \n",
              "1  <THINKING>\\nTo determine by what factor system...   \n",
              "\n",
              "                               parsed_model_response  \\\n",
              "0  {'is_valid': True, 'answer': 1, 'confidence': ...   \n",
              "1  {'is_valid': True, 'answer': 0, 'confidence': ...   \n",
              "\n",
              "                                              prompt  parsed_answer  \\\n",
              "0  Answer the following question. You must choose...              1   \n",
              "1  Answer the following question. You must choose...              0   \n",
              "\n",
              "   is_correct  \n",
              "0        True  \n",
              "1        True  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_json(results_file, lines=True)\n",
        "\n",
        "# Expand config fields into separate columns\n",
        "config_df = pd.json_normalize(df['config'])\n",
        "config_df.columns = ['config_' + col for col in config_df.columns]\n",
        "df = pd.concat([df, config_df], axis=1)\n",
        "\n",
        "# Apply filters\n",
        "df = df[(df['config_model_name'] == FILTER_MODEL_NAME) & \n",
        "        (df['config_dataset_name'] == FILTER_DATASET_NAME) & \n",
        "        (df['config_dataset_subset'] == FILTER_DATASET_SUBSET) & \n",
        "        (df['config_dataset_split'] == FILTER_DATASET_SPLIT) & \n",
        "        (df['config_random_seed'] == FILTER_RANDOM_SEED) & \n",
        "        (df['config_num_choices'] == FILTER_NUM_CHOICES)]\n",
        "\n",
        "print(f\"After filtering: {len(df)} results\")\n",
        "\n",
        "# Remove duplicates (keeping latest based on datetime)\n",
        "if REMOVE_DUPLICATES:\n",
        "    duplicate_cols = ['question_idx', 'config_model_name', 'config_dataset_name', \n",
        "                      'config_dataset_subset', 'config_dataset_split', \n",
        "                      'config_random_seed', 'config_num_choices']\n",
        "    df = df.sort_values('datetime', ascending=False)\n",
        "    df = df.drop_duplicates(subset=duplicate_cols, keep='first')\n",
        "    print(f\"After deduplication: {len(df)} results\")\n",
        "\n",
        "# Add parsed fields\n",
        "df['parsed_answer'] = df['parsed_model_response'].apply(lambda x: x.get('answer'))\n",
        "df['is_correct'] = df['parsed_answer'] == df['correct_idx']\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 100.00%\n",
            "Correct: 2/2\n"
          ]
        }
      ],
      "source": [
        "valid_df = df[df['parsed_answer'].notna()]\n",
        "accuracy = valid_df['is_correct'].mean()\n",
        "correct_count = valid_df['is_correct'].sum()\n",
        "total_count = len(valid_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Correct: {correct_count}/{total_count}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_judge_debate_NEW_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
