{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze direct QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_qa_results(filters=None):\n",
        "    df = pd.read_json(\"results/qa_results.jsonl\", lines=True)\n",
        "    \n",
        "    config_df = pd.json_normalize(df['config'])\n",
        "    config_df.columns = ['config_' + col for col in config_df.columns]\n",
        "    df = pd.concat([df, config_df], axis=1)\n",
        "\n",
        "    df['options_str'] = df['options'].apply(str)\n",
        "\n",
        "    df = df.sort_values('datetime', ascending=False).drop_duplicates(\n",
        "        subset=['question', 'options_str', 'config_model_name'], keep='first')\n",
        "\n",
        "    df['parsed_answer'] = df['parsed_model_response'].apply(lambda x: x.get('answer'))\n",
        "    df = df[df['parsed_answer'].notna()]\n",
        "\n",
        "    df['is_correct'] = df['parsed_answer'] == df['correct_idx']\n",
        "    \n",
        "    if filters:\n",
        "        for col, val in filters.items():\n",
        "            df = df[df[col] == val]\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "filters = {\n",
        "    'config_model_name': \"openai/gpt-4o-mini\",\n",
        "    # 'config_model_name': \"x-ai/grok-4-fast\",\n",
        "    'config_dataset_name': \"Idavidrein/gpqa\",\n",
        "    'config_dataset_subset': \"gpqa_diamond\",\n",
        "    'config_dataset_split': \"train\",\n",
        "    'config_random_seed': 42,\n",
        "    'config_num_choices': 2,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 56.85%\n",
            "Correct: 112/197\n"
          ]
        }
      ],
      "source": [
        "df = load_qa_results(filters)\n",
        "\n",
        "valid_df = df[df['parsed_answer'].notna()]\n",
        "accuracy = valid_df['is_correct'].mean()\n",
        "correct_count = valid_df['is_correct'].sum()\n",
        "total_count = len(valid_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Correct: {correct_count}/{total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze debate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_debate_results(run_id):\n",
        "    # Load the debate results\n",
        "    results_file = f\"results/debate/{run_id}.jsonl\"\n",
        "\n",
        "    df = pd.read_json(results_file, lines=True)\n",
        "\n",
        "    config_df = pd.json_normalize(df['config'])\n",
        "    config_df.columns = ['config_' + col for col in config_df.columns]\n",
        "    df = pd.concat([df, config_df], axis=1)\n",
        "    \n",
        "    df['options_str'] = df['options'].apply(str)\n",
        "\n",
        "    # Add is correct field\n",
        "    df['parsed_answer'] = df['judge_verdict'].apply(lambda x: x.get('parsed', {}).get('answer', {}))\n",
        "    df = df[df['parsed_answer'].notna()]\n",
        "    df['is_correct'] =  df['parsed_answer'] == df['correct_idx']\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 74.75%\n",
            "Correct: 148/198\n"
          ]
        }
      ],
      "source": [
        "run_id = 'ey2anfr'\n",
        "df = load_debate_results(run_id)\n",
        "\n",
        "valid_df = df[df['parsed_answer'].notna()]\n",
        "accuracy = valid_df['is_correct'].mean()\n",
        "correct_count = valid_df['is_correct'].sum()\n",
        "total_count = len(valid_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Correct: {correct_count}/{total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare QA and Debate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Judge QA: 56.41% (110/195)\n",
            "Debater QA: 91.79% (179/195)\n",
            "Debate Verdict: 74.87% (146/195)\n"
          ]
        }
      ],
      "source": [
        "run_id = 'ey2anfr'\n",
        "\n",
        "qa_df = load_qa_results(filters=None)\n",
        "qa_df = qa_df[['question', 'options_str', 'config_model_name', 'is_correct']]\n",
        "\n",
        "debate_df = load_debate_results(run_id)\n",
        "\n",
        "merged_df = debate_df.merge(\n",
        "    qa_df,\n",
        "    left_on=['question', 'options_str', 'config_judge_model'],\n",
        "    right_on=['question', 'options_str', 'config_model_name'],\n",
        "    how='left', suffixes=('', '_judge_qa'))\n",
        "\n",
        "merged_df = merged_df.merge(\n",
        "    qa_df,\n",
        "    left_on=['question', 'options_str', 'config_debater_model'],\n",
        "    right_on=['question', 'options_str', 'config_model_name'],\n",
        "    how='left', suffixes=('', '_debater_qa'))\n",
        "\n",
        "# only keep rows where all is_correct fields are not None\n",
        "merged_df = merged_df[merged_df['is_correct_judge_qa'].notna() & merged_df['is_correct_debater_qa'].notna() & merged_df['is_correct'].notna()]\n",
        "\n",
        "print(f'Judge QA: {merged_df[\"is_correct_judge_qa\"].mean():.2%} ({merged_df[\"is_correct_judge_qa\"].sum():.0f}/{merged_df[\"is_correct_judge_qa\"].count()})')\n",
        "print(f'Debater QA: {merged_df[\"is_correct_debater_qa\"].mean():.2%} ({merged_df[\"is_correct_debater_qa\"].sum():.0f}/{merged_df[\"is_correct_debater_qa\"].count()})')\n",
        "print(f'Debate Verdict: {merged_df[\"is_correct\"].mean():.2%} ({merged_df[\"is_correct\"].sum()}/{merged_df[\"is_correct\"].count()})')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_judge_debate_NEW_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
