{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/joshlevy/.pyenv/versions/3.11.13/envs/llm_judge_debate_NEW_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze direct QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_qa(filters=None):\n",
        "    df = pd.read_json(\"results/qa/qa_results.jsonl\", lines=True)\n",
        "    \n",
        "    config_df = pd.json_normalize(df['config'])\n",
        "    config_df.columns = ['config_' + col for col in config_df.columns]\n",
        "    df = pd.concat([df, config_df], axis=1)\n",
        "\n",
        "    df['options_str'] = df['options'].apply(str)\n",
        "\n",
        "    df = df.sort_values('datetime', ascending=False).drop_duplicates(\n",
        "        subset=['question', 'options_str', 'config_model_name'], keep='first')\n",
        "\n",
        "    df['parsed_answer'] = df['parsed_model_response'].apply(lambda x: x.get('answer'))\n",
        "    df = df[df['parsed_answer'].notna()]\n",
        "\n",
        "    df['is_correct'] = df['parsed_answer'] == df['correct_idx']\n",
        "    \n",
        "    if filters:\n",
        "        for col, val in filters.items():\n",
        "            if val is None:\n",
        "                df = df[df[col].isna()]\n",
        "            else:\n",
        "                df = df[df[col] == val]\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "filters = {\n",
        "    # MODEL OPTIONS\n",
        "    'config_model_name': \"x-ai/grok-4-fast\",\n",
        "    # 'config_model_name': \"openai/gpt-4o-mini\",\n",
        "\n",
        "    # DATASET OPTIONS\n",
        "    # 'config_dataset_name': \"Idavidrein/gpqa\",\n",
        "    # 'config_dataset_subset': \"gpqa_diamond\",\n",
        "    # 'config_dataset_split': \"train\",\n",
        "\n",
        "    'config_dataset_name': \"TIGER-Lab/MMLU-Pro\",\n",
        "    'config_dataset_subset': None,\n",
        "    'config_dataset_split': \"test\",\n",
        "\n",
        "    # OTHER OPTIONS\n",
        "    'config_random_seed': 42,\n",
        "    'config_num_choices': 4,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 90.51%\n",
            "Correct: 1773/1959\n"
          ]
        }
      ],
      "source": [
        "df = load_qa(filters)\n",
        "\n",
        "valid_df = df[df['parsed_answer'].notna()]\n",
        "accuracy = valid_df['is_correct'].mean()\n",
        "correct_count = valid_df['is_correct'].sum()\n",
        "total_count = len(valid_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Correct: {correct_count}/{total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze debate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_debate(debate_run_id):\n",
        "    df = pd.read_json(f\"results/debates/{debate_run_id}.jsonl\", lines=True)\n",
        "    config_df = pd.json_normalize(df['config'])\n",
        "    config_df.columns = ['debate_' + col for col in config_df.columns]\n",
        "    df = pd.concat([df, config_df], axis=1)\n",
        "    df['options_str'] = df['options'].apply(str)\n",
        "    return df\n",
        "\n",
        "def load_verdict(verdict_run_id):\n",
        "    df = pd.read_json(f\"results/verdicts/{verdict_run_id}.jsonl\", lines=True)\n",
        "    config_df = pd.json_normalize(df['config'])\n",
        "    config_df.columns = ['verdict_' + col for col in config_df.columns]\n",
        "    df = pd.concat([df, config_df], axis=1)\n",
        "    return df\n",
        "\n",
        "def load_debate_and_verdict(verdict_run_id):\n",
        "    verdict_df = load_verdict(verdict_run_id)\n",
        "    debate_run_id = verdict_df['debate_run_id'].iloc[0]\n",
        "    debate_df = load_debate(debate_run_id)\n",
        "    \n",
        "    df = debate_df.merge(verdict_df[['record_id', 'judge_verdict', 'verdict_judge_model']], on='record_id')\n",
        "    df['parsed_answer'] = df['judge_verdict'].apply(lambda x: x.get('parsed', {}).get('answer'))\n",
        "    df = df[df['parsed_answer'].notna()]\n",
        "    df['is_correct_verdict'] = df['parsed_answer'] == df['correct_idx']\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 56.76%\n",
            "Correct: 814/1434\n"
          ]
        }
      ],
      "source": [
        "# run_id = 'ey2anfr'\n",
        "run_id = '9amuk8w'\n",
        "\n",
        "df = load_debate_and_verdict(run_id)\n",
        "\n",
        "valid_df = df[df['parsed_answer'].notna()]\n",
        "accuracy = valid_df['is_correct_verdict'].mean()\n",
        "correct_count = valid_df['is_correct_verdict'].sum()\n",
        "total_count = len(valid_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Correct: {correct_count}/{total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare QA and Debate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_debate_and_verdict_and_qa(verdict_run_id):\n",
        "    df = load_debate_and_verdict(verdict_run_id)\n",
        "    \n",
        "    qa_df = load_qa(filters=None)\n",
        "    qa_df = qa_df[['question', 'options_str', 'config_model_name', 'is_correct']]\n",
        "    \n",
        "    judge_qa = qa_df.rename(columns={'is_correct': 'is_correct_judge_qa'})\n",
        "    df = df.merge(judge_qa, left_on=['question', 'options_str', 'verdict_judge_model'], \n",
        "                  right_on=['question', 'options_str', 'config_model_name'], how='left')\n",
        "    \n",
        "    debater_qa = qa_df.rename(columns={'is_correct': 'is_correct_debater_qa'})\n",
        "    df = df.merge(debater_qa, left_on=['question', 'options_str', 'debate_debater_model'], \n",
        "                  right_on=['question', 'options_str', 'config_model_name'], how='left')\n",
        "    \n",
        "    df = df[df['is_correct_verdict'].notna() & df['is_correct_judge_qa'].notna() & df['is_correct_debater_qa'].notna()]\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "verdict_run_id = '9amuk8w'\n",
        "\n",
        "merged_df = load_debate_and_verdict_and_qa(verdict_run_id)\n",
        "\n",
        "judge_qa = merged_df[\"is_correct_judge_qa\"].mean().round(2) \n",
        "debater_qa = merged_df[\"is_correct_debater_qa\"].mean().round(2)\n",
        "debate_verdict = merged_df[\"is_correct_verdict\"].mean().round(2)\n",
        "\n",
        "print(f'Judge QA: {judge_qa} ({merged_df[\"is_correct_judge_qa\"].sum()}/{merged_df[\"is_correct_judge_qa\"].count()})')\n",
        "print(f'Debater QA: {debater_qa} ({merged_df[\"is_correct_debater_qa\"].sum()}/{merged_df[\"is_correct_debater_qa\"].count()})')\n",
        "print(f'Debate Verdict: {debate_verdict} ({merged_df[\"is_correct_verdict\"].sum()}/{merged_df[\"is_correct_verdict\"].count()})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.bar(['Debater QA', 'Judge QA', 'Debate Verdict'], [debater_qa, judge_qa, debate_verdict], color=['#e74c3c', '#3498db', '#2ecc71'])\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "debater_model = merged_df['debate_debater_model'].iloc[0]\n",
        "judge_model = merged_df['verdict_judge_model'].iloc[0]\n",
        "plt.title(f'Verdict: {verdict_run_id}\\ndebater: {debater_model}, judge: {judge_model}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Breakdown the results by category for MMLU-Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From TIGER-Lab/MMLU-Pro, there's a column called \"category\"\n",
        "# Add this column to the merged_df\n",
        "\n",
        "verdict_run_id = '07e1puy'\n",
        "\n",
        "merged_df = load_debate_and_verdict_and_qa(verdict_run_id)\n",
        "mmlu_dataset = load_dataset('TIGER-Lab/MMLU-Pro')['test']\n",
        "\n",
        "category_map = {}\n",
        "for idx in range(len(mmlu_dataset)):\n",
        "    category_map[idx] = mmlu_dataset[idx].get('category', None)\n",
        "merged_df['category'] = merged_df['question_idx'].map(category_map)\n",
        "\n",
        "print(merged_df['category'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_results = merged_df.groupby('category').agg({\n",
        "    'is_correct_debater_qa': 'mean',\n",
        "    'is_correct_judge_qa': 'mean',\n",
        "    'is_correct_verdict': ['mean', 'count']\n",
        "}).reset_index()\n",
        "\n",
        "category_results.columns = ['category', 'is_correct_debater_qa', 'is_correct_judge_qa', 'is_correct_verdict', 'count']\n",
        "category_results = category_results.sort_values('is_correct_verdict', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "categories = category_results['category']\n",
        "counts = category_results['count']\n",
        "x = np.arange(len(categories))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, category_results['is_correct_debater_qa'], width, label='Debater QA', color='#e74c3c')                                                                                                        \n",
        "bars2 = ax.bar(x, category_results['is_correct_judge_qa'], width, label='Judge QA', color='#3498db')\n",
        "bars3 = ax.bar(x + width, category_results['is_correct_verdict'], width, label='Debate Verdict', color='#2ecc71')                                                                                                       \n",
        "\n",
        "ax.set_xlabel('Category')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title(f'Accuracy by Category\\nVerdict: {verdict_run_id}')\n",
        "ax.set_xticks(x)\n",
        "category_labels = [f'{cat} (N={cnt})' for cat, cnt in zip(categories, counts)]\n",
        "ax.set_xticklabels(category_labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCategory Results:\")\n",
        "print(category_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_results['delta'] = category_results['is_correct_debater_qa'].astype(float) - category_results['is_correct_judge_qa'].astype(float)\n",
        "category_results['gain'] = category_results['is_correct_verdict'].astype(float) - category_results['is_correct_judge_qa'].astype(float)\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "x = category_results['delta'].values.astype(float)\n",
        "y = category_results['gain'].values.astype(float)\n",
        "weights = category_results['count'].values.astype(float)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x.reshape(-1, 1), y, sample_weight=weights)\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "y_pred = model.predict(x.reshape(-1, 1))\n",
        "ss_res = np.sum(weights * (y - y_pred)**2)\n",
        "ss_tot = np.sum(weights * (y - np.average(y, weights=weights))**2)\n",
        "r_squared = 1 - (ss_res / ss_tot)\n",
        "\n",
        "pearson_r, p_value = stats.pearsonr(x, y)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "scatter = ax.scatter(category_results['delta'], category_results['gain'], \n",
        "                     c=category_results['count'], s=100, cmap='Blues', alpha=0.7, edgecolors='black')\n",
        "\n",
        "x_line = np.linspace(x.min(), x.max(), 100)\n",
        "y_line = slope * x_line + intercept\n",
        "ax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.5, label=f'Weighted fit (slope={slope:.3f})')\n",
        "\n",
        "for i, row in category_results.iterrows():\n",
        "    ax.annotate(row['category'], (row['delta'], row['gain']), \n",
        "                fontsize=8, ha='center', va='bottom', alpha=0.8)\n",
        "\n",
        "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "ax.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
        "\n",
        "stats_text = f'R² = {r_squared:.3f}\\np = {p_value:.4f}\\nslope = {slope:.3f}'\n",
        "ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "ax.set_xlabel('Delta (Debater QA - Judge QA)')\n",
        "ax.set_ylabel('Gain (Verdict - Judge QA)')\n",
        "ax.set_title(f'Debate Gain vs Debater-Judge Delta by Category\\nVerdict: {verdict_run_id}')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.legend()\n",
        "\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Number of Samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nScatter Plot Data:\")\n",
        "print(category_results[['category', 'delta', 'gain', 'count']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze token usage in the debate run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_id = 'dcg8y0b'\n",
        "\n",
        "debate_df = load_debate(run_id)\n",
        "\n",
        "record_ids = []\n",
        "completion_tokens_used_per_debate = []\n",
        "reasoning_tokens_used_per_debate = []\n",
        "for i, row in debate_df.iterrows():\n",
        "    record_ids.append(row['record_id'])\n",
        "    debate_history = row['debate_history']\n",
        "    completion_tokens_used_this_debate, reasoning_tokens_used_this_debate = 0, 0\n",
        "    for turn in debate_history:\n",
        "        completion_tokens_used_this_debate += turn['token_usage']['completion_tokens']\n",
        "        if 'completion_tokens_details' in turn['token_usage'] and turn['token_usage']['completion_tokens_details'] is not None:\n",
        "            reasoning_tokens_used_this_debate += turn['token_usage']['completion_tokens_details']['reasoning_tokens']\n",
        "        else:\n",
        "            reasoning_tokens_used_this_debate += 0\n",
        "    completion_tokens_used_per_debate.append(completion_tokens_used_this_debate)\n",
        "    reasoning_tokens_used_per_debate.append(reasoning_tokens_used_this_debate)\n",
        "\n",
        "print(record_ids)\n",
        "print(f\"completion_tokens_used_per_debate: {completion_tokens_used_per_debate}\")\n",
        "print(f\"reasoning_tokens_used_per_debate: {reasoning_tokens_used_per_debate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(np.sum(completion_tokens_used_per_debate))\n",
        "print(np.sum(reasoning_tokens_used_per_debate))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_judge_debate_NEW_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
